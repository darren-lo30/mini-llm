# Hyperparameters taken from NanoGPT
model_config: 
  vocab_size: 50304 
  embed_size: 384
  block_size: 256
  num_layers: 6
  num_attn_heads: 6
  p_dropout: 0.2
  ffn_hidden_dim_factor: 4
  bias: true
  attention_impl: 'normal'
checkpoint_file: null
batch_size: 64
num_grad_acc_steps: 1
device: 'mps'
data_path: './data'
learning_rate: 1.0e-3
num_steps: 5000
eval_freq: 100
log_freq: 10
save_freq: 500
eval_iters: 50
lr_warmup: 100
lr_decay: 5000
min_learning_rate: 1.0e-4
save_dir: './out/shakespeare'
grad_clip: 1.0